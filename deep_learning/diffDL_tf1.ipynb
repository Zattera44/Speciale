{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf2\n",
    "tf = tf2.compat.v1\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook borrows most of its code from https://github.com/differential-machine-learning/notebooks\n",
    "# The above repository was written by Anotine Savine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x_raw, y_raw, xbar_raw):\n",
    "    x_mean = x_raw.mean(axis=0)\n",
    "    x_std = x_raw.std(axis=0)\n",
    "    x = (x_raw-  x_mean) / x_std\n",
    "    y_mean = y_raw.mean(axis=0)\n",
    "    y_std = y_raw.std(axis=0)\n",
    "    y = (y_raw - y_mean) / y_std\n",
    "    xbar = xbar_raw / y_std * x_std \n",
    "\n",
    "    return x_mean, x_std, x, y_mean, y_std, y, xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twin_network(\n",
    "    input_dim,\n",
    "    n_units,\n",
    "    n_layers):\n",
    "\n",
    "    # input and initialization\n",
    "    x = tf.placeholder(shape=[None, input_dim], dtype=tf.float32)\n",
    "    z = [x]; w = [None]; b = [None]\n",
    "    w.append(tf.get_variable(\"w1\", [input_dim, n_units], initializer = tf.variance_scaling_initializer(), dtype=tf.float32))\n",
    "    b.append(tf.get_variable(\"b1\", [n_units], initializer = tf.zeros_initializer(), dtype=tf.float32))\n",
    "    z.append(z[0] @ w[1] + b[1])\n",
    "\n",
    "    # hidden layers\n",
    "    for l in range(1, n_layers): \n",
    "        w.append(tf.get_variable(\"w%d\"%(l+1), [n_units, n_units], initializer = tf.variance_scaling_initializer(), dtype=tf.float32))\n",
    "        b.append(tf.get_variable(\"b%d\"%(l+1), [n_units], initializer = tf.zeros_initializer(), dtype=tf.float32))\n",
    "        z.append(tf.nn.softplus(z[l]) @ w[l+1] + b[l+1])\n",
    "\n",
    "    # output\n",
    "    w.append(tf.get_variable(\"w\"+str(n_layers+1), [n_units, 1], initializer = tf.variance_scaling_initializer(), dtype=tf.float32))\n",
    "    b.append(tf.get_variable(\"b\"+str(n_layers+1), [1], initializer = tf.zeros_initializer(), dtype=tf.float32))\n",
    "    z.append(tf.nn.softplus(z[n_layers]) @ w[n_layers+1] + b[n_layers+1]) \n",
    "    \n",
    "    # result\n",
    "    y = z[n_layers+1]\n",
    "\n",
    "    # backpropagation\n",
    "    L = len(z) - 1\n",
    "    \n",
    "    zbar = tf.ones_like(z[L])\n",
    "    for l in range(L-1, 0, -1):\n",
    "        zbar = (zbar @ tf.transpose(w[l+1])) * tf.nn.sigmoid(z[l])\n",
    "    xbar = tf.matmul(zbar, tf.transpose(w[1]))\n",
    "\n",
    "    return x, y, xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(\n",
    "    input_dim, \n",
    "    n_units, \n",
    "    n_layers):\n",
    "\n",
    "    inputs, predictions, derivative_predictions = twin_network(input_dim, n_units, n_layers)\n",
    "    labels = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "    derivative_labels = tf.placeholder(shape=[None, derivative_predictions.shape[1]], dtype=tf.float32)\n",
    "    loss = tf.losses.mean_squared_error(labels, predictions) * tf.losses.mean_squared_error(derivative_labels, derivative_predictions)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "\n",
    "    return inputs, labels, derivative_labels, predictions, derivative_predictions, learning_rate, loss, optimizer.minimize(loss)\n",
    "\n",
    "def one_epoch(\n",
    "    inputs, labels, derivative_labels,\n",
    "    lr_placeholder, minimizer,\n",
    "    x_train, y_train, xbar_train,\n",
    "    learning_rate, batch_size, session):\n",
    "\n",
    "    m, n = x_train.shape\n",
    "    first = 0\n",
    "    last = min(batch_size, m)\n",
    "\n",
    "    while first < m:\n",
    "\n",
    "        session.run(minimizer, feed_dict = {   \n",
    "            inputs: x_train[first:last], \n",
    "            labels: y_train[first:last],\n",
    "            derivative_labels: xbar_train[first:last],\n",
    "            lr_placeholder: learning_rate\n",
    "        })\n",
    "\n",
    "        first = last\n",
    "        last = min(first + batch_size, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, x_raw, y_raw, xhat_raw):\n",
    "        self.session = None\n",
    "        self.graph = None\n",
    "        self.x_raw = x_raw\n",
    "        self.y_raw = y_raw\n",
    "        self.xhat_raw = xhat_raw\n",
    "        \n",
    "    def __del__(self):\n",
    "        if self.session is not None:\n",
    "            self.session.close()\n",
    "\n",
    "    def build(self, n_units, n_layers):\n",
    "        if self.session is not None:\n",
    "            self.session.close()\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.inputs, \\\n",
    "            self.labels, \\\n",
    "            self.derivative_labels, \\\n",
    "            self.predictions, \\\n",
    "            self.derivative_predictions, \\\n",
    "            self.learning_rate, \\\n",
    "            self.loss, \\\n",
    "            self.minimizer = train_graph(self.n, n_units, n_layers)\n",
    "\n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "\n",
    "        self.graph.finalize()\n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "\n",
    "    def prepare(self, n_units = 20, n_layers = 4):\n",
    "\n",
    "        self.x_mean, self.x_std, self.x, self.y_mean, \\\n",
    "             self.y_std, self.y, self.xbar = normalize(self.x_raw, self.y_raw, self.xhat_raw)\n",
    "        \n",
    "        self.m, self.n = self.x.shape        \n",
    "        self.build(n_units, n_layers)\n",
    "\n",
    "    def train(self,\n",
    "        learning_rate = 0.1,\n",
    "        n_epochs = 100,\n",
    "        batch_size = 256,\n",
    "        reinit=True):\n",
    "\n",
    "        if reinit:\n",
    "            self.session.run(self.initializer)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "        \n",
    "            one_epoch(\n",
    "                self.inputs, \n",
    "                self.labels, \n",
    "                self.derivative_labels,\n",
    "                self.learning_rate, \n",
    "                self.minimizer, \n",
    "                self.x, \n",
    "                self.y, \n",
    "                self.xbar, \n",
    "                learning_rate,\n",
    "                batch_size, \n",
    "                self.session)\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_scaled = (x-self.x_mean) / self.x_std\n",
    "        y_scaled, xhat_scaled = self.session.run(\n",
    "            [self.predictions, self.derivative_predictions], \n",
    "            feed_dict = {self.inputs: x_scaled})\n",
    "        y = self.y_mean + self.y_std * y_scaled\n",
    "        xhat = self.y_std / self.x_std * xhat_scaled\n",
    "        return y, xhat                                "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6314f1dd6a1867270512df5dbaf0da944fed172eba7544c32ea9917be2637492"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('thematrix')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
